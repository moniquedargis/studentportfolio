scala> import org.apache.spark.sql.types._
import org.apache.spark.sql.types._

scala> val donationsSchema = StructType(Array(StructField("ProjectID",StringType, true),StructField("DonationID",StringType,true),StructField("DonorID",StringType,true),StructField("OptionalDonation",StringType,true),StructField("Amount",FloatType,true),StructField("CartSequence",IntegerType,true),StructField("Date",StringType, true)))
donationsSchema: org.apache.spark.sql.types.StructType = StructType(StructField(ProjectID,StringType,true), StructField(DonationID,StringType,true), StructField(DonorID,StringType,true), StructField(OptionalDonation,StringType,true), StructField(Amount,FloatType,true), StructField(CartSequence,IntegerType,true), StructField(Date,StringType,true))

scala> val donationsDF = sqlContext.read.format("com.databricks.spark.csv").option("header","true").schema(donationsSchema).load("project/Donations.csv")
donationsDF: org.apache.spark.sql.DataFrame = [ProjectID: string, DonationID: string, DonorID: string, OptionalDonation: string, Amount: float, CartSequence: int, Date: string]

scala> case class Donations(ProjectID: String, DonationID: String, DonorID: String, OptionalDonation: String, Amount: Float, CartSequence: Integer, Date: String)
defined class Donations

scala> val donationsDS = donationsDF.as[Donations]
donationsDS: org.apache.spark.sql.Dataset[Donations] = [ProjectID: string, DonationID: string, DonorID: string, OptionalDonation: string, Amount: float, CartSequence: int, Date: string]

scala> val Amount = donationsDS.rdd.map(x => (x.Amount))
Amount: org.apache.spark.rdd.RDD[Float] = MapPartitionsRDD[9] at map at <console>:36

scala> Amount.stats
res0: org.apache.spark.util.StatCounter = (count: 4687884, mean: 60.668789, stdev: 166.899597, max: 60000.000000, min: 0.010000)

scala> Amount.reduce(_+_)
res1: Float = 2.8335552E8

scala> val DonorID_Amount_reduced = donationsDS.rdd.map(x => (x.DonorID, x.Amount)).reduceByKey(_+_)
DonorID_Amount_reduced: org.apache.spark.rdd.RDD[(String, Float)] = ShuffledRDD[14] at reduceByKey at <console>:36

scala> val Amount_bydonor = DonorID_Amount_reduced.map{case (x:String, y:Float) => y}
Amount_bydonor: org.apache.spark.rdd.RDD[Float] = MapPartitionsRDD[15] at map at <console>:38

scala> Amount_bydonor.stats
res2: org.apache.spark.util.StatCounter = (count: 2024554, mean: 140.479456, stdev: 2437.804472, max: 1879625.250000, min: 0.010000)

scala> val donationDate_Amount = donationsDS.rdd.map(x => (x.Date.split(" "), x.Amount)).map{case(x:Array[String], y:Float) => (x(0),y)}.reduceByKey(_+_).sortByKey(ascending=true).saveAsTextFile("project/donationDate_Amount")
donationDate_Amount: Unit = ()

scala> val donationDate_Count = donationsDS.rdd.map(x => (x.Date.split(" "))).map{case(x:Array[String]) => (x(0), 1)}.reduceByKey(_+_).sortByKey(ascending=true).saveAsTextFile("project/donationDate_Count")
donationDate_Count: Unit = ()

scala> val donorsSchema = StructType (Array(StructField("DonorID",StringType,true),StructField("City",StringType,true),StructField("State",StringType,true),StructField("IsTeacher",StringType,true),StructField("Zip",StringType,true)))
donorsSchema: org.apache.spark.sql.types.StructType = StructType(StructField(DonorID,StringType,true), StructField(City,StringType,true), StructField(State,StringType,true), StructField(IsTeacher,StringType,true), StructField(Zip,StringType,true))

scala> val donorsDF = sqlContext.read.format("com.databricks.spark.csv").option("header","true").schema(donorsSchema).load("project/Donors.csv")
donorsDF: org.apache.spark.sql.DataFrame = [DonorID: string, City: string, State: string, IsTeacher: string, Zip: string]

scala> case class Donors(DonorID: String, City: String, State: String, IsTeacher: String, Zip: String)
defined class Donors

scala> val donorsDS = donorsDF.as[Donors]
donorsDS: org.apache.spark.sql.Dataset[Donors] = [DonorID: string, City: string, State: string, IsTeacher: string, Zip: string]

scala> val DonorID_IsTeacher = donorsDS.rdd.map(x => (x.DonorID, x.IsTeacher))
DonorID_IsTeacher: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[43] at map at <console>:36

scala> DonorID_IsTeacher.count
res3: Long = 2122640

scala> val DonorID_IsTeacher_yes = DonorID_IsTeacher.filter{case (x:String, y:String) => y.contains("Yes")}
DonorID_IsTeacher_yes: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[44] at filter at <console>:38

scala> DonorID_IsTeacher_yes.count
res4: Long = 212285

scala> val DonorID_IsTeacher = donorsDS.rdd.map(x => (x.DonorID, x.IsTeacher))
DonorID_IsTeacher: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[46] at map at <console>:36

scala> val DonorID_Amount_r_IsTeacher_temp = DonorID_Amount_reduced.leftOuterJoin(DonorID_IsTeacher)
DonorID_Amount_r_IsTeacher_temp: org.apache.spark.rdd.RDD[(String, (Float, Option[String]))] = MapPartitionsRDD[49] at leftOuterJoin at <console>:48

scala> val DonorID_Amount_r_IsTeacher = DonorID_Amount_r_IsTeacher_temp.map{case(x:String, (y:Float, z: Option[String]))=> (x, (y, (z.getOrElse())))}
DonorID_Amount_r_IsTeacher: org.apache.spark.rdd.RDD[(String, (Float, Any))] = MapPartitionsRDD[50] at map at <console>:50

scala> val yes_teacher_amount = DonorID_Amount_r_IsTeacher.filter{ case (x: String, (y:Float, z:Any)) => z.toString.contains("Yes")}.map{case (x:String,(y:Float, z:Any)) => y}.stats
yes_teacher_amount: org.apache.spark.util.StatCounter = (count: 210239, mean: 290.178998, stdev: 1030.807577, max: 118311.195313, min: 1.000000)

scala> val no_teacher_amount = DonorID_Amount_r_IsTeacher.filter{ case (x: String, (y:Float, z:Any)) => !(z.toString.contains("Yes"))}.map{case (x:String,(y:Float, z:Any)) => y}.stats
no_teacher_amount: org.apache.spark.util.StatCounter = (count: 1814315, mean: 123.132588, stdev: 2550.591113, max: 1879625.250000, min: 0.010000)

scala> val DonorID_Amount = donationsDS.rdd.map(x => (x.DonorID, x.Amount))
DonorID_Amount: org.apache.spark.rdd.RDD[(String, Float)] = MapPartitionsRDD[60] at map at <console>:36

scala> val DonorID_Amount_IsTeacher_temp = DonorID_Amount.leftOuterJoin(DonorID_IsTeacher)
DonorID_Amount_IsTeacher_temp: org.apache.spark.rdd.RDD[(String, (Float, Option[String]))] = MapPartitionsRDD[63] at leftOuterJoin at <console>:48

scala> val DonorID_Amount_IsTeacher = DonorID_Amount_IsTeacher_temp.map{case(x:String, (y:Float, z: Option[String]))=> (x, (y, (z.getOrElse())))}
DonorID_Amount_IsTeacher: org.apache.spark.rdd.RDD[(String, (Float, Any))] = MapPartitionsRDD[64] at map at <console>:50

scala> val yes_teacher_amount_perdonation = DonorID_Amount_IsTeacher.filter{ case (x: String, (y:Float, z:Any)) => z.toString.contains("Yes")}.map{case (x:String,(y:Float, z:Any)) => y}
yes_teacher_amount_perdonation: org.apache.spark.rdd.RDD[Float] = MapPartitionsRDD[66] at map at <console>:52

scala> yes_teacher_amount_perdonation.stats
res5: org.apache.spark.util.StatCounter = (count: 1339221, mean: 45.554051, stdev: 215.704570, max: 21299.949219, min: 0.010000)

scala> yes_teacher_amount_perdonation.reduce(_+_)
res6: Float = 6.1006864E7

scala> val no_teacher_amount_perdonation = DonorID_Amount_IsTeacher.filter{ case (x: String, (y:Float, z:Any)) => !(z.toString.contains("Yes"))}.map{case (x:String,(y:Float, z:Any)) => y}
no_teacher_amount_perdonation: org.apache.spark.rdd.RDD[Float] = MapPartitionsRDD[70] at map at <console>:52

scala> no_teacher_amount_perdonation.stats
res7: org.apache.spark.util.StatCounter = (count: 3348663, mean: 66.713581, stdev: 142.336707, max: 60000.000000, min: 0.010000)

scala> no_teacher_amount_perdonation.reduce(_+_)
res8: Float = 2.2287504E8

scala> val OptionalDonation = donationsDS.rdd.map(x => x.OptionalDonation)
OptionalDonation: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[74] at map at <console>:36

scala> OptionalDonation.filter(x => x.contains("Yes")).count
res9: Long = 4001709

scala> val DonorID_OptionalDonation = donationsDS.rdd.map(x => (x.DonorID, x.OptionalDonation))
DonorID_OptionalDonation: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[77] at map at <console>:36

scala> val DonorID_OptionalDonation_IsTeacher_temp = DonorID_OptionalDonation.leftOuterJoin(DonorID_IsTeacher)
DonorID_OptionalDonation_IsTeacher_temp: org.apache.spark.rdd.RDD[(String, (String, Option[String]))] = MapPartitionsRDD[80] at leftOuterJoin at <console>:48

scala> val DonorID_OptionalDonation_IsTeacher = DonorID_OptionalDonation_IsTeacher_temp.map{case(x:String, (y:String, z:Option[String])) => (x, (y, (z.getOrElse())))}
DonorID_OptionalDonation_IsTeacher: org.apache.spark.rdd.RDD[(String, (String, Any))] = MapPartitionsRDD[81] at map at <console>:50

scala> val no_teacher_no_optional = DonorID_OptionalDonation_IsTeacher.filter{ case(x:String, (y:String, z:Any)) => y.contains("No") && !(z.toString.contains("Yes"))}
no_teacher_no_optional: org.apache.spark.rdd.RDD[(String, (String, Any))] = MapPartitionsRDD[82] at filter at <console>:52

scala> no_teacher_no_optional.count
res10: Long = 378272

scala> val yes_teacher_no_optional = DonorID_OptionalDonation_IsTeacher.filter{ case(x:String, (y:String, z:Any)) => y.contains("No") && (z.toString.contains("Yes"))}
yes_teacher_no_optional: org.apache.spark.rdd.RDD[(String, (String, Any))] = MapPartitionsRDD[83] at filter at <console>:52

scala> yes_teacher_no_optional.count
res11: Long = 307903

scala> val no_teacher_yes_optional = DonorID_OptionalDonation_IsTeacher.filter{ case(x:String, (y:String, z:Any)) => y.contains("Yes") && !(z.toString.contains("Yes"))}
no_teacher_yes_optional: org.apache.spark.rdd.RDD[(String, (String, Any))] = MapPartitionsRDD[84] at filter at <console>:52

scala> no_teacher_yes_optional.count
res12: Long = 2970391

scala> val yes_teacher_yes_optional = DonorID_OptionalDonation_IsTeacher.filter{ case(x:String, (y:String, z:Any)) => y.contains("Yes") && (z.toString.contains("Yes"))}
yes_teacher_yes_optional: org.apache.spark.rdd.RDD[(String, (String, Any))] = MapPartitionsRDD[85] at filter at <console>:52

scala> yes_teacher_yes_optional.count
res13: Long = 1031318

scala> val projectsSchema = StructType(Array(StructField("ProjectID",StringType,true),StructField("SchoolID",StringType,true),StructField("TeacherID",StringType,true),StructField("Sequence",IntegerType,true),StructField("ProjectType",StringType,true),StructField("Title",StringType,true),StructField("Essay",StringType,true),StructField("ShortDescription",StringType,true),StructField("NeedStatement",StringType,true),StructField("SubjectCategory",StringType,true),StructField("SubjectSubcategory",StringType,true),StructField("GradeLevel",StringType,true),StructField("ResourceCategory",StringType,true),StructField("Cost",FloatType,true),StructField("PostedDate",StringType,true),StructField("ExpirationDate",StringType,true),StructField("CurrentStatus",StringType,true),StructField("FullyFundedDate",StringType,true)))
projectsSchema: org.apache.spark.sql.types.StructType = StructType(StructField(ProjectID,StringType,true), StructField(SchoolID,StringType,true), StructField(TeacherID,StringType,true), StructField(Sequence,IntegerType,true), StructField(ProjectType,StringType,true), StructField(Title,StringType,true), StructField(Essay,StringType,true), StructField(ShortDescription,StringType,true), StructField(NeedStatement,StringType,true), StructField(SubjectCategory,StringType,true), StructField(SubjectSubcategory,StringType,true), StructField(GradeLevel,StringType,true), StructField(ResourceCategory,StringType,true), StructField(Cost,FloatType,true), StructField(PostedDate,StringType,true), StructField(ExpirationDate,StringType,true), StructField(CurrentStatus,StringType,true), StructField(FullyFu...
scala> val projectsDF = sqlContext.read.format("com.databricks.spark.csv").option("header","true").option("mode","DROPMALFORMED").schema(projectsSchema).load("project/Projects.csv")
projectsDF: org.apache.spark.sql.DataFrame = [ProjectID: string, SchoolID: string, TeacherID: string, Sequence: int, ProjectType: string, Title: string, Essay: string, ShortDescription: string, NeedStatement: string, SubjectCategory: string, SubjectSubcategory: string, GradeLevel: string, ResourceCategory: string, Cost: float, PostedDate: string, ExpirationDate: string, CurrentStatus: string, FullyFundedDate: string]

scala> case class Projects(ProjectID: String, SchoolID: String, TeacherID: String, Sequence: String, ProjectType: String, Title: String, Essay: String, ShortDescription: String, NeedStatement: String, SubjectCategory: String, SubjectSubcategory: String, GradeLevel: String, ResourceCategory: String, Cost: Float, PostedDate: String, ExpirationDate: String, CurrentStatus: String, FullyFundedDate: String)
defined class Projects

scala> val projectsDS = projectsDF.as[Projects]
projectsDS: org.apache.spark.sql.Dataset[Projects] = [ProjectID: string, SchoolID: string, TeacherID: string, Sequence: string, ProjectType: string, Title: string, Essay: string, ShortDescription: string, NeedStatement: string, SubjectCategory: string, SubjectSubcategory: string, GradeLevel: string, ResourceCategory: string, Cost: float, PostedDate: string, ExpirationDate: string, CurrentStatus: string, FullyFundedDate: string]

scala> val projectsEssay_Status = projectsDS.rdd.map(x => (x.Essay, x.CurrentStatus))
projectsEssay_Status: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[9] at map at <console>:36

scala> projectsEssay_Status.count
res0: Long = 355954

scala> val projectsWC_Funded = projectsEssay_Status.filter{case (x:String,y:String) => y.contains("Fully Funded")}.map{case(x:String,y:String) => x}.flatMap{case (x:String) => x.split("\\W+")}.filter{case (x:String) => !(x.contains("DONOTREMOVEESSAYDIVIDER"))}.filter(_.length!=0).filter(_(0).isLetter).map(x => (x.toLowerCase, 1)).reduceByKey(_+_).sortBy(-_._2).saveAsTextFile("project/projectsWC_Funded")
projectsWC_Funded: Unit = ()

scala> val projectsWC_NotFunded = projectsEssay_Status.filter{case (x:String,y:String) => !(y.contains("Fully Funded"))}.map{case(x:String,y:String) => x}.flatMap{case (x:String) => x.split("\\W+")}.filter{case (x:String) => !(x.contains("DONOTREMOVEESSAYDIVIDER"))}.filter(_.length!=0).filter(_(0).isLetter).map(x => (x.toLowerCase, 1)).reduceByKey(_+_).sortBy(-_._2).saveAsTextFile("project/projectsWC_NotFunded")
projectsWC_NotFunded: Unit = ()
